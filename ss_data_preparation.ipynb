{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Airbnb Price Prediction - Data Preparation\n",
    "\n",
    "Use dataset published by Kaggle - https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data - to train a simple deep learning model to predict prices for Airbnb properties.\n",
    "\n",
    "\n",
    "This notebook contains the common data loading and preparation steps:\n",
    "- load data from the input CSV\n",
    "- do an assessment of the dataset to understand the number of distinct, missing, or invalid values by column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common imports and variables\n",
    "Imports and variable definitions that are common to the entire notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import logging\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_file):\n",
    "    ''' open config file with name config_file that contains parameters\n",
    "    for this module and return Python object\n",
    "\n",
    "    Args:\n",
    "        config_file: filename containing config parameters\n",
    "\n",
    "    Returns:\n",
    "        config: Python dictionary with config parms from config file - dictionary\n",
    "\n",
    "\n",
    "    '''\n",
    "    current_path = os.getcwd()\n",
    "    path_to_yaml = os.path.join(current_path, config_file)\n",
    "    print(\"path_to_yaml \" + path_to_yaml)\n",
    "    try:\n",
    "        with open(path_to_yaml, 'r') as c_file:\n",
    "            config = yaml.safe_load(c_file)\n",
    "        return config\n",
    "    except Exception as error:\n",
    "        print('Error reading the config file ' + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_values(config):\n",
    "    #1. All JSON keys/vals are strings, no conversions needed\n",
    "    #2. Even better: Use f-strings: PEP 498, since Py 3.6 \n",
    "    for val in config:\n",
    "        print(f\"config value {val} = {config[val]}\" )\n",
    "#         print(\"config value \",val,\" \",str(config[val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "- ingest CSV into a Pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    ''' get the path for data files\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "\n",
    "    '''\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory\n",
    "    # containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'data'))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(path,input_csv,pickled_input_dataframe,save_raw_dataframe,load_from_scratch):\n",
    "    ''' load data into dataframe\n",
    "    Args:\n",
    "        path: path containing input file\n",
    "        input_csv: input file name\n",
    "        pickled_input_dataframe: pickled version of input file\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "    '''\n",
    "    if load_from_scratch:\n",
    "        # if loading from scratch, the raw CSV file is expected to be in the data directory which is a sibling to the \n",
    "        # directory that contains this notebook\n",
    "        unpickled_df = pd.read_csv(os.path.join(path,input_csv)) \n",
    "        if save_raw_dataframe:\n",
    "            file_name = os.path.join(path,pickled_input_dataframe)\n",
    "            print(\"file_name is \",file_name)\n",
    "            unpickled_df.to_pickle(file_name)\n",
    "    else:\n",
    "        unpickled_df = pd.read_pickle(os.path.join(path,pickled_input_dataframe))\n",
    "        logging.debug(\"reloader done\")\n",
    "    return(unpickled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess values\n",
    "- assess columns for missing or invalid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_in_list(x, list):\n",
    "    ''' check if a value is in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "        list: list in which to check for the value\n",
    "\n",
    "    Returns:\n",
    "        return_val: 1 if value is in not in list, 0 otherwise\n",
    "    '''\n",
    "    return x not in list\n",
    "# \n",
    "#     if x in list:\n",
    "#         return_val = 0\n",
    "#     else:\n",
    "#         return_val = 1\n",
    "#     return(return_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_val(x):\n",
    "    ''' check if a value is negative #in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "    \n",
    "    Returns:\n",
    "        return_val: 1 if value is negative, 0 otherwise\n",
    "    '''\n",
    "    return x < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_assessment(df,columns,valid_values,non_neg_continuous):\n",
    "    ''' assess the values in a dataframe\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        columns: dictionary of column names by category\n",
    "        valid_values: dictionary of valid values for categorical columns with limited number of valid values\n",
    "        non_neg_continuous: list of continuous columns with only non-negative values as valid\n",
    "    '''\n",
    "# No need to wrap the df in a list to get the columns! But look out below! \n",
    "# columns is also the YML field passed in!      \n",
    "    for col in df.columns:\n",
    "#         print(\"Missing values in \",col,\" \",str(df[col].isna().sum()))\n",
    "#         print(\"Distinct values in \",col,\" \",str(df[col].nunique()))\n",
    "        print(\"Missing values in \",col,\" \",df[col].isna().sum())\n",
    "        print(\"Distinct values in \",col,\" \", df[col].nunique())\n",
    "\n",
    "    # for categorical columns with a limited number of valid values, count the number of invalid values by column\n",
    "    for col in valid_values:\n",
    "#        print(\"non-valid values in column \",col,\" \",str(df[col].apply(lambda x:not_in_list(x,valid_values[col])).sum()))\n",
    "         print(\"non-valid values in column \",col,\" \", df[col].apply(lambda x:not_in_list(x,valid_values[col])).sum())\n",
    "    \n",
    "    # count non-numeric values in continuous columns\n",
    "    for col in columns['continuous']:\n",
    "        mask = pd.to_numeric(df[col], errors='coerce').isna()\n",
    "        # count non-numeric values in continuous columns\n",
    "        print(\"non-numeric values in continuous col \",col,\" \", mask.sum())\n",
    "        # if there are no non-numeric values in the column and it must have non-negative values, count negative values\n",
    "        if (mask.sum()==0) and (col in non_neg_continuous):\n",
    "#           print(\"negative values in column \",col,\" \",str(df[col].apply(lambda x:neg_val(x)).sum()))\n",
    "            print(\"negative values in column \",col,\" \", df[col].apply(lambda x:neg_val(x)).sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_range(x,max,min):\n",
    "    ''' count whether a value is in a range\n",
    "    Args:\n",
    "        x: value to check in range\n",
    "        max: top of the range to check\n",
    "        min: bottom of the range to check\n",
    "        \n",
    "    Returns:\n",
    "        ret_val: 1 if out of range, 0 otherwise\n",
    "    '''\n",
    "    return x > max or x < min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_bounding_box(latitude,longitude,bounding_box):\n",
    "    ''' count whether a location is within a bounding box\n",
    "        NOTE: Also checks for invalid lat/longs, if any.\n",
    "        TODO: Since lat/long are Series objects, might be \n",
    "        more optimal to combine them into a df and then\n",
    "        have apply return a bunch of tuples?\n",
    "    Args:\n",
    "        latitude: latitude portion of location\n",
    "        longitude: longitude portion of location\n",
    "        bounding_box: dictionary with max and min values to compare the location with\n",
    "        min: bottom of the range to check\n",
    "        \n",
    "    Returns:\n",
    "        ret_val: 1 if out of range, 0 otherwise\n",
    "    '''    \n",
    "    min_lat =  bounding_box['min_lat']\n",
    "    max_lat =  bounding_box['max_lat']\n",
    "    min_long = bounding_box['min_long']\n",
    "    max_long = bounding_box['max_long']\n",
    "# The apply() calls below take about 10X as long!!!    \n",
    "#     t1 = time.perf_counter_ns()\n",
    "#     df = pd.merge(latitude, longitude, left_index = True, right_index = True)\n",
    "#     print(f\"Merge of 2 pd.Series(.) took {time.perf_counter_ns() - t1} ns.\")\n",
    "#     t1 = time.perf_counter_ns()\n",
    "#     d1 = df.apply(lambda x: (x.latitude > 90) + (x.latitude < -90) + (x.longitude > 180) +\n",
    "#          (x.longitude < -180) + (x.latitude > max_lat) + (x.latitude < min_lat) + (x.longitude > max_long) +\n",
    "#          (x.longitude < min_long), axis = 1, result_type='expand').sum() \n",
    "#     print(f\"type(d1) = {type(d1)}, Invalid lats calc took {time.perf_counter_ns() - t1} ns.\")\n",
    "#     t1 = time.perf_counter_ns()\n",
    "    \n",
    "# #     print(df.apply(lambda x: (x.latitude > 90) + (x.latitude < -90) + (x.longitude > 180) +\n",
    "# #          (x.longitude < -180) + (x.latitude > max_lat) + (x.latitude < min_lat) + (x.longitude > max_long) +\n",
    "# #          (x.longitude < min_long), axis = 1, result_type='expand').sum(), f\" <= Invalid lats: calc took {time.perf_counter_ns() - t1} ns.\")\n",
    "#     d2 = df.apply(lambda x: (x.latitude > 90) + (x.latitude < -90) + (x.longitude > 180) +\n",
    "#          (x.longitude < -180) + (x.latitude > max_lat) + (x.latitude < min_lat) + (x.longitude > max_long) +\n",
    "#          (x.longitude < min_long), axis = 1, result_type='expand').sum() #, f\" <= Invalid lats: calc took {time.perf_counter_ns() - t1} ns.\")\n",
    "#     print(f\"type(d2) = {type(d2)}, Invalid lats calc 2 took {time.perf_counter_ns() - t1} ns.\")\n",
    "    t1 = time.perf_counter_ns()\n",
    "    total = (\n",
    "#         latitude[latitude > 90].sum() + #alternately\n",
    "        sum(latitude > 90)\n",
    "        + sum(latitude < -90) + sum(longitude > 180) \n",
    "        + sum (longitude < -180) + sum(latitude > max_lat) + sum(latitude < min_lat) \n",
    "        + sum(longitude > max_long) + sum(longitude < min_long)\n",
    "    )   \n",
    "    print(f\"lat/long checks took {time.perf_counter_ns() - t1} ns.\")   \n",
    "    return total                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_assessment(df,bounding_box):\n",
    "    ''' assess the geo columns in a dataframe by counting how many latitude and longitude values are outside the bounding box\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        bounding_box: dictionary of maximum and minimum valid latitude and longitude values\n",
    "    ''' \n",
    "    oobb = out_of_bounding_box(df.latitude,df.longitude,bounding_box)\n",
    "    print(\"location out of bounds count \",oobb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:logging check\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path is  /media/srutis/Acer/Users/SRUTIS/projects/Manning/DLAirBnBPrices-LP/data\n",
      "path_to_yaml /media/srutis/Acer/Users/SRUTIS/projects/Manning/DLAirBnBPrices-LP/solution_milestone1/data_preparation_config.yml\n",
      "config value general = {'load_from_scratch': True, 'save_raw_dataframe': True, 'save_transformed_dataframe': True, 'remove_bad_values': True}\n",
      "config value columns = {'categorical': ['neighbourhood_group', 'neighbourhood', 'room_type'], 'continuous': ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'latitude', 'longitude'], 'date': ['last_review'], 'text': ['name', 'host_name'], 'excluded': ['id', 'hostid']}\n",
      "config value category_defaults = {'categorical': 'missing', 'continuous': 0.0, 'text': 'missing', 'date': datetime.date(2019, 1, 1), 'excluded': 'missing'}\n",
      "config value category_invalid_value_replacements = {'categorical': 'bad_categorical', 'continuous': 'bad_continuous', 'text': 'bad_text', 'date': 'bad_date', 'exclude': 'bad_excluded'}\n",
      "config value latitude_replacement = bad_latitude\n",
      "config value longitude_replacement = bad_longitude\n",
      "config value non_negative_continuous = ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count']\n",
      "config value valid_values = {'neighbourhood_group': ['Bronx', 'Brooklyn', 'Queens', 'Manhattan', 'Staten Island'], 'room_type': ['Private room', 'Shared room', 'Entire home/apt']}\n",
      "config value bounding_box = {'max_long': -73.70018092, 'max_lat': 40.91617849, 'min_long': -74.25909008, 'min_lat': 40.47739894}\n",
      "config value newark_bounding_box = {'max_long': -74.11278706, 'max_lat': 40.67325015, 'min_long': -74.25132408, 'min_lat': 40.78813864}\n",
      "config value geo_columns = ['latitude', 'longitude']\n",
      "config value file_names = {'input_csv': 'AB_NYC_2019.csv', 'pickled_input_dataframe': 'AB_NYC_2019_input_aug16_2020.pkl', 'pickled_output_dataframe': 'AB_NYC_2019_output_aug20_2020.pkl'}\n",
      "file_name is  /media/srutis/Acer/Users/SRUTIS/projects/Manning/DLAirBnBPrices-LP/data/AB_NYC_2019_input_aug16_2020.pkl\n",
      "Missing values in  id   0\n",
      "Distinct values in  id   48895\n",
      "Missing values in  name   16\n",
      "Distinct values in  name   47905\n",
      "Missing values in  host_id   0\n",
      "Distinct values in  host_id   37457\n",
      "Missing values in  host_name   21\n",
      "Distinct values in  host_name   11452\n",
      "Missing values in  neighbourhood_group   0\n",
      "Distinct values in  neighbourhood_group   5\n",
      "Missing values in  neighbourhood   0\n",
      "Distinct values in  neighbourhood   221\n",
      "Missing values in  latitude   0\n",
      "Distinct values in  latitude   19048\n",
      "Missing values in  longitude   0\n",
      "Distinct values in  longitude   14718\n",
      "Missing values in  room_type   0\n",
      "Distinct values in  room_type   3\n",
      "Missing values in  price   0\n",
      "Distinct values in  price   674\n",
      "Missing values in  minimum_nights   0\n",
      "Distinct values in  minimum_nights   109\n",
      "Missing values in  number_of_reviews   0\n",
      "Distinct values in  number_of_reviews   394\n",
      "Missing values in  last_review   10052\n",
      "Distinct values in  last_review   1764\n",
      "Missing values in  reviews_per_month   10052\n",
      "Distinct values in  reviews_per_month   937\n",
      "Missing values in  calculated_host_listings_count   0\n",
      "Distinct values in  calculated_host_listings_count   47\n",
      "Missing values in  availability_365   0\n",
      "Distinct values in  availability_365   366\n",
      "non-valid values in column  neighbourhood_group   0\n",
      "non-valid values in column  room_type   0\n",
      "non-numeric values in continuous col  minimum_nights   0\n",
      "negative values in column  minimum_nights   0\n",
      "non-numeric values in continuous col  number_of_reviews   0\n",
      "negative values in column  number_of_reviews   0\n",
      "non-numeric values in continuous col  reviews_per_month   10052\n",
      "non-numeric values in continuous col  calculated_host_listings_count   0\n",
      "negative values in column  calculated_host_listings_count   0\n",
      "non-numeric values in continuous col  latitude   0\n",
      "non-numeric values in continuous col  longitude   0\n",
      "Merge of 2 pd.Series(.) took 2025186 ns.\n",
      "lat/long checks took 126673922 ns.\n",
      "location out of bounds count  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Master cell\n",
    "#This cell contains calls to the other functions in this notebook to complete the data preparation\n",
    "\n",
    "# master cell to call the other functions\n",
    "# get the path for data files\n",
    "path = get_path()\n",
    "print(\"path is \",path)\n",
    "config = get_config('data_preparation_config.yml')\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.warning(\"logging check\")\n",
    "print_config_values(config)\n",
    "# load dataframe and, if parameter set, save CSV file as a pickled dataframe\n",
    "df = ingest_data(path,config['file_names']['input_csv'],config['file_names']['pickled_input_dataframe'],config['general']['save_raw_dataframe'],config['general']['load_from_scratch'])\n",
    "# get basic assessment information for the dataframe\n",
    "basic_assessment(df,config['columns'],config['valid_values'],config['non_negative_continuous'])\n",
    "# get assessment for geospatial information\n",
    "geo_assessment(df,config['bounding_box'])\n",
    "df.head()\n",
    "samp = df.sample(frac=.001, random_state=42)\n",
    "samp.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
