{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Airbnb Price Prediction - Data Cleanup - HELP VERSION\n",
    "\n",
    "Use dataset published by Kaggle - https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data - to train a simple deep learning model to predict prices for Airbnb properties.\n",
    "\n",
    "\n",
    "This notebook contains the data cleanup steps:\n",
    "- load data from the input CSV or dataframe saved by the [data preparation notebook](https://github.com/ryanmark1867/end_to_end_deep_learning_liveproject/blob/master/notebooks/data_preparation.ipynb)\n",
    "- fix missing values\n",
    "- clean up anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common imports and variables\n",
    "Imports and variable definitions that are common to the entire notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import datetime as dt\n",
    "# # common imports\n",
    "# import zipfile\n",
    "# import time\n",
    "# # import datetime, timedelta\n",
    "# import datetime\n",
    "# from datetime import datetime, timedelta\n",
    "# from datetime import date\n",
    "# from dateutil import relativedelta\n",
    "# from io import StringIO\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# from sklearn.base import BaseEstimator\n",
    "# from sklearn.base import TransformerMixin\n",
    "# from io import StringIO\n",
    "# import requests\n",
    "# import json\n",
    "# from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline \n",
    "# import os\n",
    "# import math\n",
    "# from subprocess import check_output\n",
    "# from IPython.display import display\n",
    "# import logging\n",
    "# import yaml\n",
    "# from collections import Counter\n",
    "# import re\n",
    "# import os\n",
    "# import numbers\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import logging\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_file):\n",
    "    ''' open config file with name config_file that contains parameters\n",
    "    for this module and return Python object\n",
    "\n",
    "    Args:\n",
    "        config_file: filename containing config parameters\n",
    "\n",
    "    Returns:\n",
    "        config: Python dictionary with config parms from config file - dictionary\n",
    "\n",
    "\n",
    "    '''\n",
    "    current_path = os.getcwd()\n",
    "    path_to_yaml = os.path.join(current_path, config_file)\n",
    "    print(\"path_to_yaml \" + path_to_yaml)\n",
    "    try:\n",
    "        with open(path_to_yaml, 'r') as c_file:\n",
    "            config = yaml.safe_load(c_file)\n",
    "        return config\n",
    "    except Exception as error:\n",
    "#       print('Error reading the config file ' + str(error))\n",
    "        print('Error reading the config file ' + error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_values(config):\n",
    "    ''' print values in dictionary config'''\n",
    "    for val in config:\n",
    "        print(f\"config value {val} = {config[val]}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataframe\n",
    "- load pickled dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    ''' get the path for data files\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "\n",
    "    '''\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory\n",
    "    # containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'data'))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(path,input_csv,pickled_input_dataframe,save_raw_dataframe,load_from_scratch):\n",
    "    ''' load data into dataframe\n",
    "    Args:\n",
    "        path: path containing input file\n",
    "        input_csv: input file name\n",
    "        pickled_input_dataframe: pickled version of input file\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "    '''\n",
    "    if load_from_scratch:\n",
    "        # if loading from scratch, the raw CSV file is expected to be in the data directory which is a sibling to the \n",
    "        # directory that contains this notebook\n",
    "        unpickled_df = pd.read_csv(os.path.join(path,input_csv)) \n",
    "        if save_raw_dataframe:\n",
    "            file_name = os.path.join(path,pickled_input_dataframe)\n",
    "            print(\"file_name is \",file_name)\n",
    "            unpickled_df.to_pickle(file_name)\n",
    "    else:\n",
    "        unpickled_df = pd.read_pickle(os.path.join(path,pickled_input_dataframe))\n",
    "        logging.debug(\"reloader done\")\n",
    "    return(unpickled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(dataset,columns,defaults):\n",
    "    ''' replace missing values with placeholders by column type\n",
    "    \n",
    "    Args:\n",
    "        dataset: dataframe in which missing values being processed\n",
    "        columns: dictionary of columns with keys that are column types and values that are the column names of that type\n",
    "        defaults: dictionary of replacement values for missing values by column type\n",
    "\n",
    "    Returns:\n",
    "        dataset: dataframe with missing values replaced with default values\n",
    "         \n",
    "    '''\n",
    "    for ctype in columns: #.keys():\n",
    "        for cname in columns[ctype]:\n",
    "            print(f\"# missing values BEFORE cleaning ds[{cname}] = {dataset[cname].isna().sum()}\")\n",
    "            dataset[cname].fillna(defaults[ctype], inplace=True)        \n",
    "            print(f\"# missing values AFTER cleaning ds[{cname}] = {dataset[cname].isna().sum()}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_in_list(x, list):\n",
    "    ''' check if a value is in a list\n",
    "    Args:%ls ../../data\n",
    "        x: value to check\n",
    "        list: list in which to check for the value\n",
    "\n",
    "    Returns:\n",
    "        retur_val: 1 if value is in not in list, 0 otherwise\n",
    "    '''\n",
    "    return not x in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_val(x):\n",
    "    ''' check if a value is in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "    \n",
    "    Returns:\n",
    "        retur_val: 1 if value is negative, 0 otherwisnamee\n",
    "    '''\n",
    "    return x < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_assessment(df,columns,valid_values,non_neg_continuous):\n",
    "    ''' assess the values in a dataframe\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        columns: dictionary of column names by category\n",
    "        valid_values: dictionary of valid values for categorical columns with limited number of valid values\n",
    "        non_neg_continuous: list of continuous columns with only non-negative values as valid\n",
    "    '''\n",
    "# No need to wrap the df in a list to get the columns! But look out below! \n",
    "# columns is also the YML field passed in!  \n",
    "    for col in df.columns:\n",
    "        print(\"Missing values in \",col,\" \",df[col].isna().sum())\n",
    "        print(\"Distinct values in \",col,\" \", df[col].nunique())\n",
    "\n",
    "    # for categorical columns with a limited number of valid values, count the number of invalid values by column\n",
    "    for col in valid_values:\n",
    "         print(\"non-valid values in column \",col,\" \", df[col].apply(lambda x:not_in_list(x,valid_values[col])).sum())\n",
    "    \n",
    "    # count non-numeric values in continuous columns\n",
    "    for col in columns['continuous']:\n",
    "        mask = pd.to_numeric(df[col], errors='coerce').isna()\n",
    "        # count non-numeric values in continuous columns\n",
    "        print(\"non-numeric values in continuous col \",col,\" \", mask.sum())\n",
    "        # if there are no non-numeric values in the column and it must have non-negative values, count negative values\n",
    "        if (mask.sum()==0) and (col in non_neg_continuous):\n",
    "            print(\"negative values in column \",col,\" \", df[col].apply(lambda x:neg_val(x)).sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_range(x,max,min):\n",
    "    ''' count whether a value is in a range\n",
    "    Args:\n",
    "        x: value to check in range\n",
    "        max: top of the range to check\n",
    "        min: bottom of the range to check\n",
    "        \n",
    "    Returns:\n",
    "        ret_val: 1 if out of range, 0 otherwise\n",
    "    '''\n",
    "    return x < min or x > max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_bounding_box(latitude,longitude,bounding_box):\n",
    "    ''' count whether a location is within a bounding box\n",
    "        NOTE: Also checks for invalid lat/longs, if any.\n",
    "        TODO: Since lat/long are Series objects, might be \n",
    "        more optimal to combine them into a df and then\n",
    "        have apply return a bunch of tuples?\n",
    "    Args:\n",
    "        latitude: latitude portion of location\n",
    "        longitude: longitude portion of location\n",
    "        bounding_box: dictionary with max and min values to compare the location with\n",
    "        min: bottom of the range to check\n",
    "        \n",
    "    Returns:\n",
    "        count of OOBB/invalid lats/longs\n",
    "        #ret_val: 1 if out of range, 0 otherwise\n",
    "    '''    \n",
    "    min_lat =  bounding_box['min_lat']\n",
    "    max_lat =  bounding_box['max_lat']\n",
    "    min_long = bounding_box['min_long']\n",
    "    max_long = bounding_box['max_long']  \n",
    "    \n",
    "    t1 = time.perf_counter_ns()\n",
    "                            \n",
    "    total = (\n",
    "#         latitude[latitude > 90].sum() + #alternately\n",
    "        sum(latitude > 90)\n",
    "        + sum(latitude < -90) + sum(longitude > 180) \n",
    "        + sum (longitude < -180) + sum(latitude > max_lat) + sum(latitude < min_lat) \n",
    "        + sum(longitude > max_long) + sum(longitude < min_long)\n",
    "    )   \n",
    "    print(f\"lat/long checks took {time.perf_counter_ns() - t1} ns.\")   \n",
    "    return total        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_assessment(df,bounding_box):\n",
    "    ''' assess the geo columns in a dataframe by counting how many latitude and longitude values are outside the bounding box\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        bounding_box: dictionary of maximum and minimum valid latitude and longitude values\n",
    "    ''' \n",
    "    oobb = out_of_bounding_box(df.latitude,df.longitude,bounding_box)\n",
    "    print(\"location out of bounds count \", oobb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_non_numeric(x, replace_x):\n",
    "    ''' check if a value is non-numeric and replace if so\n",
    "    Args:\n",
    "        x: value to check\n",
    "    \n",
    "    Returns:\n",
    "        retur_val: 1 if value is negative, 0 otherwise\n",
    "    '''\n",
    "    if pd.to_numeric(x, errors='coerce').isna():\n",
    "        return replace_x\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_not_in_list(x, replace_x, list):\n",
    "    ''' check if a value is in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "        list: list in which to check for the value\n",
    "\n",
    "    Returns:\n",
    "        retur_val: 1 if value is in not in list, 0 otherwise\n",
    "    '''\n",
    "    if x not in list:\n",
    "        return replace_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_neg(x, replace_x):\n",
    "    ''' check if a value is negative and replace if so\n",
    "    Args:\n",
    "        x: value to check\n",
    "    \n",
    "    Returns:\n",
    "        retur_val: 1 if value is negative, 0 otherwise\n",
    "    '''\n",
    "    if x < 0:\n",
    "        return replace_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_replacement(df,replace_lat, replace_long, bounding_box):\n",
    "    ''' assess the geo columns in a dataframe by counting how many latitude and longitude values are outside the bounding box\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        replace_lat: replacement for latitude if out of bounds\n",
    "        replace_long: replacement for longitude if out of bounds\n",
    "        bounding_box: dictionary of maximum and minimum valid latitude and longitude values\n",
    "    Returns:\n",
    "        df: updated dataframe\n",
    "    ''' \n",
    "    min_lat =  bounding_box['min_lat']\n",
    "    max_lat =  bounding_box['max_lat']\n",
    "    min_long = bounding_box['min_long']\n",
    "    max_long = bounding_box['max_long']\n",
    "    df['latitude','longitude'] = df.apply(lambda x:replace_if_outside_bounding_box(x.latitude,x.longitude,replace_lat,replace_long,bounding_box),axis=1)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_outside_bounding_box(latitude,longitude, replace_lat, replace_long, bounding_box):\n",
    "    ''' count whether a location is within a bounding box\n",
    "    Args:\n",
    "        latitude: latitude portion of location\n",
    "        longitude: longitude portion of location\n",
    "        replace_lat: replacement value for latitude\n",
    "        replace_long: replacement value for longitude\n",
    "        bounding_box: dictionary with max and min values to compare the location with\n",
    "               \n",
    "    Returns:\n",
    "        latitude, longitude: 1 if out of range, 0 otherwise\n",
    "    ''' \n",
    "    min_lat =  bounding_box['min_lat']\n",
    "    max_lat =  bounding_box['max_lat']\n",
    "    min_long = bounding_box['min_long']\n",
    "    max_long = bounding_box['max_long']\n",
    "    lat = latitude\n",
    "    if latitude < -90 or latitude > 90 or latitude > max_lat or latitude < min_lat:\n",
    "        lat = replace_lat\n",
    "    long = longitude    \n",
    "    if longitude < -180 or  longitude > 180 or longitude > max_long or longitude < min_long:\n",
    "        long = replace_long\n",
    "    return lat, long\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_time(date_time_value,time_value):\n",
    "    ''' given a datetime replace the time portion '''\n",
    "    date_time_value = date_time_value.replace(hour=time_value.hour,minute=time_value.minute,second=time_value.minute)\n",
    "    return date_time_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_invalid_values(df,columns,valid_values,invalid_value_replacements,non_neg_continuous):\n",
    "    ''' replace invalid with placeholders\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        columns: dictionary of column names by category\n",
    "        valid_values: dictionary of valid values for categorical columns with limited number of valid values\n",
    "        invalid_value_replacements: dictionary of replacement values by column category\n",
    "        non_neg_continuous: list of continuous columns with only non-negative values as valid\n",
    "    Returns:\n",
    "        df: updated dataframe\n",
    "    '''\n",
    "    repl_cat = invalid_value_replacements[\"categorical\"]\n",
    "    repl_real = invalid_value_replacements[\"continuous\"]\n",
    "    \n",
    "    for col in columns[\"categorical\"]:\n",
    "        df[col] = df[col].apply(lambda x: replace_if_not_in_list(x, repl_cat, valid_values))  \n",
    "\n",
    "    for col in columns[\"continuous\"] + columns[\"excluded\"]:\n",
    "        if col in non_neg_continuous:\n",
    "            df[col] = df[col].apply(lambda x: replace_if_neg(x, repl_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master cell\n",
    "This cell contains calls to the other functions in this notebook to complete the data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:logging check\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path is  /media/srutis/Acer/Users/SRUTIS/projects/Manning/DLAirBnBPrices-LP/project1-prep-tab-data/data\n",
      "path_to_yaml /media/srutis/Acer/Users/SRUTIS/projects/Manning/DLAirBnBPrices-LP/project1-prep-tab-data/ms2/data_preparation_config.yml\n",
      "past config definition\n",
      "config value general = {'load_from_scratch': True, 'save_raw_dataframe': True, 'save_transformed_dataframe': True, 'remove_bad_values': True}\n",
      "config value columns = {'categorical': ['neighbourhood_group', 'neighbourhood', 'room_type'], 'continuous': ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'latitude', 'longitude'], 'date': ['last_review'], 'text': ['name', 'host_name'], 'excluded': ['price', 'id']}\n",
      "config value category_defaults = {'categorical': 'missing', 'continuous': 0.0, 'text': 'missing', 'date': datetime.date(2019, 1, 1), 'excluded': 'missing'}\n",
      "config value category_invalid_value_replacements = {'categorical': 'bad_categorical', 'continuous': 'bad_continuous', 'text': 'bad_text', 'date': 'bad_date', 'exclude': 'bad_excluded'}\n",
      "config value latitude_replacement = bad_latitude\n",
      "config value longitude_replacement = bad_longitude\n",
      "config value non_negative_continuous = ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count']\n",
      "config value valid_values = {'neighbourhood_group': ['Bronx', 'Brooklyn', 'Queens', 'Manhattan', 'Staten Island'], 'room_type': ['Private room', 'Shared room', 'Entire home/apt']}\n",
      "config value bounding_box = {'max_long': -73.70018092, 'max_lat': 40.91617849, 'min_long': -74.25909008, 'min_lat': 40.47739894}\n",
      "config value newark_bounding_box = {'max_long': -74.11278706, 'max_lat': 40.67325015, 'min_long': -74.25132408, 'min_lat': 40.78813864}\n",
      "config value geo_columns = ['latitude', 'longitude']\n",
      "config value file_names = {'input_csv': 'AB_NYC_2019.csv', 'pickled_input_dataframe': 'AB_NYC_2019_input_aug16_2020.pkl', 'pickled_output_dataframe': 'AB_NYC_2019_output_aug20_2020.pkl'}\n",
      "file_name is  /media/srutis/Acer/Users/SRUTIS/projects/Manning/DLAirBnBPrices-LP/project1-prep-tab-data/data/AB_NYC_2019_input_aug16_2020.pkl\n",
      "columns are {'categorical': ['neighbourhood_group', 'neighbourhood', 'room_type'], 'continuous': ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'latitude', 'longitude'], 'date': ['last_review'], 'text': ['name', 'host_name'], 'excluded': ['price', 'id']}.\n",
      "category_defaults is {'categorical': 'missing', 'continuous': 0.0, 'text': 'missing', 'date': datetime.date(2019, 1, 1), 'excluded': 'missing'}\n",
      "# missing values BEFORE cleaning ds[neighbourhood_group] = 0\n",
      "# missing values AFTER cleaning ds[neighbourhood_group] = 0\n",
      "# missing values BEFORE cleaning ds[neighbourhood] = 0\n",
      "# missing values AFTER cleaning ds[neighbourhood] = 0\n",
      "# missing values BEFORE cleaning ds[room_type] = 0\n",
      "# missing values AFTER cleaning ds[room_type] = 0\n",
      "# missing values BEFORE cleaning ds[minimum_nights] = 0\n",
      "# missing values AFTER cleaning ds[minimum_nights] = 0\n",
      "# missing values BEFORE cleaning ds[number_of_reviews] = 0\n",
      "# missing values AFTER cleaning ds[number_of_reviews] = 0\n",
      "# missing values BEFORE cleaning ds[reviews_per_month] = 10052\n",
      "# missing values AFTER cleaning ds[reviews_per_month] = 0\n",
      "# missing values BEFORE cleaning ds[calculated_host_listings_count] = 0\n",
      "# missing values AFTER cleaning ds[calculated_host_listings_count] = 0\n",
      "# missing values BEFORE cleaning ds[latitude] = 0\n",
      "# missing values AFTER cleaning ds[latitude] = 0\n",
      "# missing values BEFORE cleaning ds[longitude] = 0\n",
      "# missing values AFTER cleaning ds[longitude] = 0\n",
      "# missing values BEFORE cleaning ds[last_review] = 10052\n",
      "# missing values AFTER cleaning ds[last_review] = 0\n",
      "# missing values BEFORE cleaning ds[name] = 16\n",
      "# missing values AFTER cleaning ds[name] = 0\n",
      "# missing values BEFORE cleaning ds[host_name] = 21\n",
      "# missing values AFTER cleaning ds[host_name] = 0\n",
      "# missing values BEFORE cleaning ds[price] = 0\n",
      "# missing values AFTER cleaning ds[price] = 0\n",
      "# missing values BEFORE cleaning ds[id] = 0\n",
      "# missing values AFTER cleaning ds[id] = 0\n",
      "Missing values in  id   0\n",
      "Distinct values in  id   48895\n",
      "Missing values in  name   0\n",
      "Distinct values in  name   47906\n",
      "Missing values in  host_id   0\n",
      "Distinct values in  host_id   37457\n",
      "Missing values in  host_name   0\n",
      "Distinct values in  host_name   11453\n",
      "Missing values in  neighbourhood_group   0\n",
      "Distinct values in  neighbourhood_group   5\n",
      "Missing values in  neighbourhood   0\n",
      "Distinct values in  neighbourhood   221\n",
      "Missing values in  latitude   0\n",
      "Distinct values in  latitude   19048\n",
      "Missing values in  longitude   0\n",
      "Distinct values in  longitude   14718\n",
      "Missing values in  room_type   0\n",
      "Distinct values in  room_type   3\n",
      "Missing values in  price   0\n",
      "Distinct values in  price   674\n",
      "Missing values in  minimum_nights   0\n",
      "Distinct values in  minimum_nights   109\n",
      "Missing values in  number_of_reviews   0\n",
      "Distinct values in  number_of_reviews   394\n",
      "Missing values in  last_review   0\n",
      "Distinct values in  last_review   1765\n",
      "Missing values in  reviews_per_month   0\n",
      "Distinct values in  reviews_per_month   938\n",
      "Missing values in  calculated_host_listings_count   0\n",
      "Distinct values in  calculated_host_listings_count   47\n",
      "Missing values in  availability_365   0\n",
      "Distinct values in  availability_365   366\n",
      "non-valid values in column  neighbourhood_group   0\n",
      "non-valid values in column  room_type   0\n",
      "non-numeric values in continuous col  minimum_nights   0\n",
      "negative values in column  minimum_nights   0\n",
      "non-numeric values in continuous col  number_of_reviews   0\n",
      "negative values in column  number_of_reviews   0\n",
      "non-numeric values in continuous col  reviews_per_month   0\n",
      "negative values in column  reviews_per_month   0\n",
      "non-numeric values in continuous col  calculated_host_listings_count   0\n",
      "negative values in column  calculated_host_listings_count   0\n",
      "non-numeric values in continuous col  latitude   0\n",
      "non-numeric values in continuous col  longitude   0\n",
      "lat/long checks took 74419997 ns.\n",
      "location out of bounds count  0\n",
      "path is  /media/srutis/Acer/Users/SRUTIS/projects/Manning/DLAirBnBPrices-LP/project1-prep-tab-data/data\n",
      "file_name is  /media/srutis/Acer/Users/SRUTIS/projects/Manning/DLAirBnBPrices-LP/project1-prep-tab-data/data/AB_NYC_2019_output_aug20_2020.pkl\n",
      "All done  in 1519007898 ns!!!\n"
     ]
    }
   ],
   "source": [
    "# master cell to call the other functions\n",
    "# get the path for data files\n",
    "t1 = time.perf_counter_ns()\n",
    "path = get_path()\n",
    "print(\"path is \",path)\n",
    "config = get_config('data_preparation_config.yml')\n",
    "print(\"past config definition\")\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.warning(\"logging check\")\n",
    "print_config_values(config)\n",
    "\n",
    "# Refactored\n",
    "column_config = config['columns']\n",
    "defaults_config = config['category_defaults']\n",
    "filename_config = config['file_names']\n",
    "validvalues_config = config['valid_values']\n",
    "non_neg_config = config['non_negative_continuous']\n",
    "invalid_value_config = config['category_invalid_value_replacements']\n",
    "\n",
    "# load dataframe\n",
    "df = ingest_data(path,filename_config['input_csv'],filename_config['pickled_input_dataframe'],config['general']['save_raw_dataframe'],config['general']['load_from_scratch'])\n",
    "\n",
    "# TODO! ONLY for test!!!!\n",
    "#df = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# print(\"columns is \"+str(config['columns']))\n",
    "# print(\"category_defaults is \"+str(config['category_defaults']))\n",
    "print(f\"columns are {column_config}.\")\n",
    "print(f\"category_defaults is {defaults_config}\")\n",
    "# fill missing values according to the defaults per column\n",
    "fill_missing(df, column_config, defaults_config)\n",
    "\n",
    "# get assessment results after filling missing values\n",
    "basic_assessment(df, column_config, validvalues_config, non_neg_config)\n",
    "# df = replace_invalid_values(df,columns,valid_values,invalid_value_replacements,non_neg_continuous)\n",
    "#df = \n",
    "replace_invalid_values(df,column_config, validvalues_config, invalid_value_config, non_neg_config)\n",
    "geo_assessment(df,config['bounding_box'])\n",
    "'''\n",
    "df = geo_replacement(df,config['latitude_replacement'],config['longitude_replacement'], config['bounding_box'])\n",
    "'''\n",
    "if config['general']['save_transformed_dataframe']:\n",
    "    print(\"path is \",path)    \n",
    "    file_name = os.path.join(path,config['file_names']['pickled_output_dataframe'])\n",
    "    print(\"file_name is \",file_name)\n",
    "    df.to_pickle(file_name)\n",
    "df.head()\n",
    "print(f\"All done  in {time.perf_counter_ns() - t1} ns!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
